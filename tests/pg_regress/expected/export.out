-- pg_regress test for compliance export (User Story 4: T052-T054)
-- Tests that history table can be exported for audit purposes
-- Insert test data for export testing
INSERT INTO walrus.history
    (timestamp, action, old_size_mb, new_size_mb, forced_checkpoints, checkpoint_timeout_sec, reason, metadata)
VALUES
    ('2025-01-15 10:00:00+00', 'increase', 1024, 2048, 5, 300, 'Forced checkpoints exceeded threshold', '{"delta": 5, "multiplier": 6}'::jsonb),
    ('2025-01-15 11:00:00+00', 'decrease', 2048, 1536, 0, 300, 'Sustained low activity', '{"shrink_factor": 0.75}'::jsonb),
    ('2025-01-15 12:00:00+00', 'capped', 2048, 4096, 10, 300, 'Calculated size exceeded walrus.max', '{"walrus_max_mb": 4096}'::jsonb);
-- Verify data exists
SELECT count(*) AS records_for_export FROM walrus.history;
 records_for_export 
--------------------
                  3
(1 row)

-- Test export query format (spec acceptance scenario)
-- Exclude id column as it's non-deterministic (auto-increment sequence)
SELECT timestamp, action, old_size_mb, new_size_mb, forced_checkpoints, checkpoint_timeout_sec, reason, metadata
FROM walrus.history
WHERE timestamp >= '2025-01-01'
ORDER BY timestamp;
          timestamp           |  action  | old_size_mb | new_size_mb | forced_checkpoints | checkpoint_timeout_sec |                reason                 |           metadata            
------------------------------+----------+-------------+-------------+--------------------+------------------------+---------------------------------------+-------------------------------
 Wed Jan 15 02:00:00 2025 PST | increase |        1024 |        2048 |                  5 |                    300 | Forced checkpoints exceeded threshold | {"delta": 5, "multiplier": 6}
 Wed Jan 15 03:00:00 2025 PST | decrease |        2048 |        1536 |                  0 |                    300 | Sustained low activity                | {"shrink_factor": 0.75}
 Wed Jan 15 04:00:00 2025 PST | capped   |        2048 |        4096 |                 10 |                    300 | Calculated size exceeded walrus.max   | {"walrus_max_mb": 4096}
(3 rows)

-- Test that JSONB metadata is preserved in queries (T054)
-- Verify we can access specific metadata fields
SELECT
    action,
    metadata->>'delta' AS delta,
    metadata->>'shrink_factor' AS shrink_factor,
    metadata->>'walrus_max_mb' AS walrus_max_mb
FROM walrus.history
ORDER BY timestamp;
  action  | delta | shrink_factor | walrus_max_mb 
----------+-------+---------------+---------------
 increase | 5     |               | 
 decrease |       | 0.75          | 
 capped   |       |               | 4096
(3 rows)

-- Test COPY TO format (cannot actually write file in pg_regress, but verify query works)
-- The actual COPY command would be:
-- COPY (SELECT * FROM walrus.history WHERE timestamp >= '2025-01-01' ORDER BY timestamp) TO '/tmp/audit.csv' WITH CSV HEADER;
-- We test the SELECT portion (excluding id for deterministic output):
SELECT
    timestamp AT TIME ZONE 'UTC' AS timestamp_utc,
    action,
    old_size_mb,
    new_size_mb,
    forced_checkpoints,
    checkpoint_timeout_sec,
    reason,
    metadata::text AS metadata_json
FROM walrus.history
ORDER BY timestamp;
      timestamp_utc       |  action  | old_size_mb | new_size_mb | forced_checkpoints | checkpoint_timeout_sec |                reason                 |         metadata_json         
--------------------------+----------+-------------+-------------+--------------------+------------------------+---------------------------------------+-------------------------------
 Wed Jan 15 10:00:00 2025 | increase |        1024 |        2048 |                  5 |                    300 | Forced checkpoints exceeded threshold | {"delta": 5, "multiplier": 6}
 Wed Jan 15 11:00:00 2025 | decrease |        2048 |        1536 |                  0 |                    300 | Sustained low activity                | {"shrink_factor": 0.75}
 Wed Jan 15 12:00:00 2025 | capped   |        2048 |        4096 |                 10 |                    300 | Calculated size exceeded walrus.max   | {"walrus_max_mb": 4096}
(3 rows)

-- Verify all column types are export-friendly
SELECT
    column_name,
    data_type,
    CASE
        WHEN data_type IN ('bigint', 'integer', 'text', 'timestamp with time zone', 'jsonb') THEN 'CSV compatible'
        ELSE 'Check compatibility'
    END AS export_status
FROM information_schema.columns
WHERE table_schema = 'walrus' AND table_name = 'history'
ORDER BY ordinal_position;
      column_name       |        data_type         | export_status  
------------------------+--------------------------+----------------
 id                     | bigint                   | CSV compatible
 timestamp              | timestamp with time zone | CSV compatible
 action                 | text                     | CSV compatible
 old_size_mb            | integer                  | CSV compatible
 new_size_mb            | integer                  | CSV compatible
 forced_checkpoints     | bigint                   | CSV compatible
 checkpoint_timeout_sec | integer                  | CSV compatible
 reason                 | text                     | CSV compatible
 metadata               | jsonb                    | CSV compatible
(9 rows)

-- Clean up
DELETE FROM walrus.history;
